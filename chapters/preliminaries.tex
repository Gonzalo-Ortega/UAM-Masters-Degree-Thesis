\chapter{Preliminaries}
The contents of this chapter are based on \cite{burago}, \cite{nanda}, \cite{polterovich} and \cite{wang}.

\section{Persistence modules and interleaving distance}

\begin{definition}[Graded ring]
    Let $ R $ be a ring. It is said that $ R $ is a {\bf graded ring} if it can be decomposed into a direct sum of additive groups
    $$
        R = \bigoplus_{n=1}^{\infty} R_n = R_1 \oplus R_2 \oplus \dots
    $$
    such that for all $ n, m \geq 0 $, 
    $$
        R_n R_m = R_{n+m}.
    $$
\end{definition}

\begin{definition}[Graded ideal]
    Let $ R $ be a graded ring. A {\bf graded ideal} is a two sided ideal $ I \subseteq R $ that can be decomposed into a direct sum
    $$
        I = \bigoplus_{n=1}^{\infty} I_n
    $$
    where each $n \geq 0 $, $ I_n = I \cap R_n $.
\end{definition}

\begin{definition}[Left moudule, Definition IV.1.1.1 \cite{hungerford}]
    Let $ R $ be a ring. A {\bf left $R$-module } is an abelian group $ (M, +) $ with an operation $ \cdot \colon R \times M \to M $ such that for all $ r, s \in R $ and for all $ x, y \in M $,
    \begin{enumerate}
    \renewcommand{\labelenumi}{(\roman{enumi})}
        \item $ (rs) \cdot x = r (s \cdot x) $,
        \item $ (r + s) = r \cdot x + s \cdot x $,
        \item $ r \cdot (x + y) = r \cdot x + r \cdot y $.
    \end{enumerate}
    If $ R $ has a multiplicative identity $ 1 $, then $ M $ is said to be a {\bf unitary $R$-module} and 
    \begin{enumerate}
    \renewcommand{\labelenumi}{(\roman{enumi})}
        \setcounter{enumi}{3}
        \item $1 \cdot x = x $.
    \end{enumerate}
    If $ R $ is a division ring, that is, a ring with identity where every non cero element is a unit, then 
    a unitary $R$-module is called a {\bf left $R$-vector space}. Note that in this case, $R$ is in fact a field.
\end{definition}

\begin{definition}[Graded moudule, Definition 4.7 \cite{wang}]
    Let $M$ be a left module over a graded ring $ R $. It is said that $ M $ is a {\bf left graded module} if it can be decomposed into a direct sum
    $$
        M = \bigoplus_{n=1}^{\infty} M_n
    $$
    if for each $n, m \geq 0 $, $ R_n M_m \subseteq M_{n+m} $.
\end{definition}

\begin{definition}[Persistence module]
    Let $/F$ be a field and let $T$ be a totally ordered set. Let $ V = \{V_t\}_{t \in T} $ be a collection of $F$-vector spaces. A $T$-indexed {\bf persistence module} is a pair $ (V, \pi) $ such that $ \pi = \{ \pi_{s \leq t} \} $ is a collection of linear maps $ \pi_{s \leq t}\colon V_s \rightarrow V_t $ that verifies that for all $ r, s, t \in T $,
    \begin{equation}
        \pi_{r \leq s} \circ \pi_{s \leq t} = \pi_{r \leq t}.
    \end{equation}
\end{definition}

\begin{definition}[Morphism between persistence modules]
    Let $T$ be a totally ordered set. Let $ (V, \pi), (W, \theta) $ be two persistence modules. A {\bf morphism} between persistence modules $ p \colon (V, \pi) \to (W, \theta) $ is a family of linear maps $ p_t \colon V_t \to W_t $ such that for all $ s \leq t $ the following diagram commutes:
    $$
    \begin{tikzcd}
        V_s \arrow[r, "\pi_{s \leq t}"] \arrow[d, "p_s"'] & V_t \arrow[d, "p_t"] \\
        W_s \arrow[r, "\theta_{s \leq t}"']               & W_t
    \end{tikzcd}
    $$
    If a morphism $ i $ verifies that for all $ t \in T $, $ i_t \colon V_t \to V_t $ is the identity, then $ i $ is the {\bf identity morphism}. If there exists two morphisms $ p \colon (V, \pi) \to (W, \theta) $ and $ q \colon (W, \theta) \to (V, \pi) $ such that the compositions $ p \circ q $ and $ q \circ p $ are both the identity morphism, then $ p $ and $ q $ are {\bf isomorphisms} of persistence modules. In this case, $ (V, \pi) $ and $ (W, \theta) $ are said to be {\bf isomorphic} persistence modules.
\end{definition}

For now on, to simplify notation, we will limit our totally order set to be the real numbers, $ T = \R $. Also, when there is no possible confusion, we might denote the persistence module $ (V, \pi) $ by just is collection of vector spaces $ V $.

\begin{definition}[Persistence module shift]
    Let $ (V, \pi) $ be a persistence module and let $ \delta \in \R $. The {\bf $\delta$-shift} of $ (V, \pi) $ is the persistence module $ (V_\delta, \pi_\delta) $ defined by taking
    \begin{align}
        (V_\delta)_t &\coloneq V_{t+\delta}, \quad & (\pi_\delta)_{s\leq t} &\coloneq \pi_{s+\delta \leq t+\delta}.
    \end{align}
\end{definition}

\begin{proposition}[Exercise 1.2.3, \cite{polterovich}] \label{prop:shift-morphism}
    Let $ \delta > 0 $. Let $(V, \pi), (V_\delta, \pi_\delta) $ be a persistence module and its shift. The map $ \phi_\delta \colon (V, \pi) \to (V_\delta, \pi_\delta) $, defined as
    $$
        \phi_\delta (V_t) \coloneq \pi_{t \leq t + \delta}(V_t) = V_{t+\delta},
    $$
    is a persistence module morphism.
\end{proposition}
\begin{proof}
    As $ \delta > 0 $, then $ t \leq t+\delta $. Hence
    \begin{align}
        \phi_\delta \circ \pi_{t \leq t+\delta} (V_t) &= \phi_\delta (V_{t+\delta}) = V_{t+\delta+\delta} = V_{t+2 \delta}, \\
        \pi_{t+\delta \leq t+2\delta} \circ \phi_\delta (V_t) &= \pi_{t+\delta \leq t+2\delta}(V_{t+\delta}) = V_{t+2 \delta}.
    \end{align}
\end{proof}

\begin{definition}[Shift morphism]
    The persistence module morphism $ \phi_\delta $ defined as in Proposition \ref{prop:shift-morphism} is named {\bf $\delta$-shift morphism}.
\end{definition}

\begin{definition}[$\delta$-interleaved modules]
    Let $ (V, \pi), (W, \theta) $ be two persistence modules and let $ \delta > 0 $. $ V $ and $ W $ are {\bf $\delta$-interleaved } if there exists two persistence module morphisms $ \phi \colon V \to W_\delta $ and $ \psi \colon W \to V_\delta $ such that the following diagrams commute:
    $$
    \begin{array}{ccc}
    \begin{tikzcd}
        V \arrow[r, "\phi"] \arrow[rr, "\pi_{2\delta}"', bend right] & W_\delta \arrow[r, "\psi_\delta"] & V_{2\delta}
    \end{tikzcd},
    &
    \quad \quad
    &
    \begin{tikzcd}
        W \arrow[r, "\psi"] \arrow[rr, "\theta_{2\delta}"', bend right] & V_\delta \arrow[r, "\phi_\delta"] & W_{2\delta}
    \end{tikzcd}.
    \end{array}
    $$
\end{definition}

Persistence modules are a vast abstract algebraic tool. In order to make it more manejable, we give it some more structure, restricting the dimension of the vector spaces, Also, we limit as the amount of different up to isomorphism vector spaces there are.

\begin{definition}[Tame persistence module]
    A a persistence module $ (V, \pi) $ over $ \R $ is {\bf tame} if
    \begin{enumerate}
        \renewcommand{\labelenumi}{(\roman{enumi})}
        \item For all $ t \geq 0 $, $ \dim(V_t) $ is finite.
        \item For any $\varepsilon > 0 $, there exists a finite subset $ K \subset \R $ such that for all $ t \in \R \setminus K $, the map $ \pi_{t-\varepsilon \leq t+\varepsilon} \colon V_{t-\varepsilon} \to V_{t+\varepsilon} $ is not an isomorphism.
    \end{enumerate}
\end{definition}

\begin{definition}[Interleaving distance]
    Let $ (V, \pi) $ and $ (W, \theta) $ be tame two persistence modules. The {\bf interleaving distance} between them is defined as
    $$
        \di(V, W) \coloneq \inf \{ \delta > 0 \mid V \text{ and } W \text{ are } \delta\text{-interleaved}\}.
    $$
\end{definition}

\begin{proposition}
    The interleaving distance between two tame persistence modules is actually a distance.
\end{proposition}

\begin{definition}[Interval module]
    Let $ I = (a, b] $ be an interval with $ b \leq \infty $ and let $ \F $ be a field. An {\bf interval module} $\F(I)$ is a persistence module defined as
    \begin{align}
        & \F(I)_t \coloneq \begin{cases}
            &\F \text{ if } t \in I, \\
            &0 \text{ else}, 
        \end{cases}
        &
        & \pi_{s \leq t} = \begin{cases}
            & \id \text{ if } t \in I, \\
            &0 \text{ else}.
        \end{cases}
    \end{align}
\end{definition}

\begin{definition}[Direct sum of persistance modules]
    Let $ (V, \pi) $ and $ (V', \pi') $ be two persistence modules. Their {\bf direct sum} $ (W, \theta) $ is a persistence module where
    \begin{align}
        W_t &\coloneq V_t \oplus V_t', \text{ the direct sum of both vector spaces, and} \\
        \theta_{s \leq t} &\coloneq \pi_{s \leq t} \oplus \pi'_{s \leq t}.
    \end{align}
\end{definition}

\section{Barcodes and the bottleneck distance}

\begin{definition}[Barcode]
    A {\bf barcode} $B$ is a finite multiset of intervals. That is, a collection $\{(I_i, m_i)\}$ of intervals $I_i$ with multiplicities $m_i \in N$, where each interval $ I_i $ is either finite of the form $(a, b]$ or infinite of the form $(a, \infty)$. Each interval $I_i$ is named to be a {\bf bar} of $B$.
\end{definition}

Given an interval $ I = (a, b]$, and some $ \delta \geq 0 $, we will denote
\begin{equation}
    I^\delta \coloneq (a-\delta, b+\delta].
\end{equation}
We will denote the strict upper triangular region of the Euclidean plane as
\begin{equation}
   \upr := \{(x, y) \in \R^2 : x < y\}, 
\end{equation}
and the diagonal of the plane as
\begin{equation}
    \Delta := \{(x, y) \in \R^2 : x = y\}.
\end{equation}

\begin{definition}[Multiset matching]
    Let $ X $ and $ Y $ be two multi-sets and let $ X' \subseteq X $, $ Y' \subseteq Y$. A {\bf matching} between them is a bijection $ \mu \colon X' \to Y' $. The elements in $ X' $ and $ Y' $ are said to be {\bf matched} by $ \mu $.
\end{definition}

Note that $ X' = \coim(\mu) $ and $ Y' = \im(\mu) $. Also note that as $ X $ and $ Y $ are multisets, it might happen that one same element appears several times in one of the multisets, ans that some, but not all of its copies are matched to some element in the other multiset. 

\begin{definition}[$\delta$-matching barcodes] \label{delta-matching}
    A delta matching between two barcodes $ B $ and $ C $ is a multiset matching that verifies
    \begin{enumerate}
        \item $B_{2\delta} \subseteq \coim(\mu) $,
        \item $C_{2\delta} \subseteq \im(\mu) $,
        \item If $\mu(I) = J$, then $I \subseteq J^\delta$ and $J \subseteq I^\delta$.
    \end{enumerate}
\end{definition}

There are various ways of defining the bottleneck distance, all of them equivalent to one an other. We first give the natural definition that comes up following the use of $\delta$-matchings.

\begin{definition}[Bottleneck distance]
    The {\bf bottleneck distance} between two barcodes $ B $ and $ C $ is the infimum over all $ \delta \in \R $ such that there exists a $\delta$-matching between $ B $ and $ C $.
\end{definition}

\section{Persistence diagrams and the Wasserstein distance} \label{sec:wp-persistance}
We will denote the strict upper triangular region of the Euclidean plane as
\begin{equation}
    \upr := \{(x, y) \in \R^2 : x < y\},
\end{equation}
and the diagonal of the plane as
\begin{equation}
  \Delta := \{(x, y) \in \R^2 : x = y\}.
\end{equation}

\begin{definition}[Persistence diagram]
    Let $ I $ be a countable set. A {\it persistence diagram} is a function $ D: I \to \upr $.
\end{definition}

Persistence diagrams are just a way of presenting the out coming from computing the persistence homology groups of a set of data. This output comes in the so called {\it barcodes}, which are multisets of intervals. As every interval is given with its {\it birth} and {\it death} parameters, it can as well be seen as a point in $ \upr $.

\begin{definition}[Partial matching]
    Let $ D_1: I_1 \to \upr $ and $ D_2: I_2 \to \upr $ be persistence diagrams. A {\it partial matching} between $ D_1 $ and $ D_2 $ is the triple $ (I_1', I_2', f) $ such that $ f: I_1' \to I_2' $ is a bijection with $ I_1' \subseteq I_1 $ and $ I_2' \subseteq I_2 $.
\end{definition}

Instead of probability measures, now we are actually dealing with countable sets of points in $ \R $. We will make use of the $ l^p $ norm at countable spaces to measure the distance between matched pairs and the distance between unmatched pairs and the diagonal $ \Delta $. For a more detailed explanation of Lebesgue measures check \cite{rudin}[Definition 3.7]. This norm is named after Pafnuty Chebyshev.

\begin{definition}[Chebyshev distance]
    Let $ a, b \in \R^2 $ with $a = (a_x, a_y) $ and $ b = (b_x, b_y) $. The {\it Chebyshev distance} is defined as
    \begin{equation}
        d_\infty(a, b) := ||a-b||_{\infty} := \max \{|a_x - b_x|, |a_y - b_y|\}.
    \end{equation}
\end{definition}

To define our adapted Wasserstein distance we need to check how Chebyshev distance measures distances between points of $ \upr $ and $ \Delta $.

\begin{proposition} \label{prop:distance-delta}
    If $ a = (a_x, a_y) \in \upr $, then $ d_\infty(a, \Delta) = \inf_{t \in \Delta} d_\infty(a, t) = \frac{a_y - a_x}{2} $.
\end{proposition}
\begin{proof}
    The $ t $ which minimizes the distance is the midpoint of $ a_x $ and $ a_y $, that is $t = \left(\frac{a_x+a_y}{2}, \frac{a_x+a_y}{2}\right)  $. Then,
    \begin{equation}
        \left| a_x - \frac{a_x+a_y}{2} \right| = \left| \frac{a_x-a_y}{2}\right| = \left| \frac{a_y-a_x}{2}\right| = \left| a_y - \frac{a_x+a_y}{2} \right|,
    \end{equation}
    and as $ a_y > a_x $ we have
    \begin{equation}
        d_\infty(a, t) = \left|\frac{a_y - a_x}{2}\right| = \frac{a_y - a_x}{2}.
    \end{equation}
\end{proof}

We now verify that the upper triangular region of the Euclidean plane with the Chebyshev distance adapted to measure distances in $ \Delta $ is a metric space.
\begin{proposition}
    The function $ d_\infty $ is a distance in $ \upr $ with the diagonal $ \Delta $.
\end{proposition}
\begin{proof}
    For points $ a, b \in \upr \subset \R^2 $, $ d_\infty $ is a distance as usual Lebesgue norms are well defined. See \cite{rudin}[Chapter 3]. To verify that the metric requirements are fulfilled for $ d_\infty(a, \Delta) $, it is enough to consider $ t = \frac{a_y - a_x}{2} $ as in Proposition \ref{prop:distance-delta}.
\end{proof}

\begin{definition}[$p$-cost] \label{def:pcost}
    Let $ D_1: I_1 \to \upr $ and $ D_2: I_2 \to \upr $ be persistence diagrams. Let $ (I_1', I_2', f) $ be a partial matching between them. If $ p < \infty $, the {\it $p$-cost of $ f $} is defined as
    \begin{align*}
        \costp(f) := \bigg(&\sum_{i \in I_1'} d_\infty(D_1(i), D_2(f(i)))^p \\
        &+ \sum_{i \in I_1 \setminus I_1'} d_\infty(D_1(i), \Delta)^p \\
        &+ \sum_{i \in I_2 \setminus I_2'} d_\infty(D_2(i), \Delta)^p \bigg)^{\frac{1}{p}}.
    \end{align*}
    For $ p = \infty $, the {\it $\infty$-cost of $ f $} is defined as
    \begin{align*}
        \costi(f) := \max \bigg\{&\sup_{i \in I_1'} d_\infty(D_1(i), D_2(f_i)), \\
        &\sup_{i\in I_1 \setminus I_1'} d_\infty(D_1(i), \Delta), \\
        &\sup_{i\in I_2 \setminus I_2'} d_\infty(D_2(i), \Delta)\bigg\}.
    \end{align*}
\end{definition}

\begin{definition}[p-Wasserstein distance] \label{def:Wasserstein}
    Let $ D_1, D_2 $ be persistence diagrams. Let $ 1 \leq p \leq \infty $. Define
    $$
        \twdp (D_1, D_2) = \inf \{\costp(f) : f \text{ is a partial matching between } D_1 \text{ and } D_2 \}.
    $$
    Let $ \emptyset $ denote the unique persistence diagram with empty indexing set. Let $ (\dgmp, \wdp) $ be the space of persistence diagrams $ D $ that satisfy $ \twdp(D, \emptyset) < \infty $ modulo the equivalence relation $ D_1 \sim D_2 $ if $ \twdp (D_1, D_2) = 0 $. The metric $ \wdp $ is called the {\it $p$-Wasserstein distance}.
\end{definition}

\begin{definition}[Bottleneck distance]
    In the conditions of Definition \ref{def:Wasserstein}, if $ p = \infty $, the metric $ \wdi $ is called the {\it bottleneck distance}.
\end{definition}

\begin{proposition} \label{prop-empty-mathing-distance}
    There is only one matching between $ D: I \to \upr $ and $ \emptyset $. Hence, if $ p \leq \infty $,
    \begin{equation}
        \twdp(D, \emptyset) = \left(\sum_{i\in I} d_\infty(D(i), \Delta)^p\right)^{\frac{1}{p}},
    \end{equation}
    and, if $ p = \infty $,
    \begin{equation}
        \twdi(D, \emptyset) = \sup_{i\in I} d_\infty(D_1(i), \Delta)
    \end{equation}
\end{proposition}
\begin{proof}
    Let $ I' \subseteq D $. If $ f $ is a partial matching between $ D $ and $ \emptyset $, means that $ f(I') = \emptyset$ is a bijection. That is only posible if $ I' = \emptyset $ too. Therefore $ I \setminus I' = I \setminus \emptyset = I $ and following Definition \ref{def:pcost} we conclude our proof.
\end{proof}

Next proposition will prove that, in indeed, the space of persistence diagrams with the $p$-Wasserstein distance $(\dgmp, \wdp)$ is a metric space. Its proof is usually omitted in literature, as it based on the simple fact that $ d_\infty $ is a distance. We will give, however, an step by step version here.

\begin{proposition}
    $\wdp$ is a distance on the space $ (\dgmp, \wdp) $.
\end{proposition}
\begin{proof}
    Let $ D_1, D_2, D_3 \in \dgmp$, with $ 1 \leq p \leq \infty $. First of all, $ \wdp (D_1, D_2) \geq 0 $ because $ d_\infty \geq 0 $. $ \wdp (D_1, D_2) = 0 $ if and only if $ \twdp (D_1, D_2) = 0 $. Thus, because of the equivalence relationship used to define $ \wdp $, it has to be $ D_1 \sim D_2 $.

    To check symmetry, note that every partial matching $ f $ is bijective, therefore $ f^{-1} $ is a partial matching. But, for all $ i \in I_1'$, exists $ j \in I_2' $ such that $ f(i) = j $ and
    \begin{align}
        d_\infty (D_1(i), D_2(f(i))) = d_\infty (D_2(f(i)), D_1(i)) = d_\infty (D_2(j), D_1(f^{-1}(j))).
    \end{align}
    Then, $ \costp(f) = \costp(f^{-1}) $ and we have
    \begin{align}
        \wdp (D_1, D_2) &= \inf \{\costp(f) : f \text{ is a partial matching between } D_1 \text{ and } D_2 \} \\
        &= \inf \{\costp(f^{-1}) : f^{-1} \text{ is a partial matching between } D_2 \text{ and } D_1 \} \\
        &= \wdp (D_2, D_1).
    \end{align}
    
    Finally, lets prove the triangle inequality. If $ f: I_1' \to I_2' $ is a partial matching between $ D_1 $ and $ D_2 $ and $ g: I_2' \to I_3' $ is a partial matching between $ D_2 $ and $ D_3 $, $ g \circ f: I_1' \to I_3' $ is a partial matching between $ D_1 $ and $ D_3 $ as both $ f $ and $ g $ are bijective. Computing the cost of the matchings for $ p < \infty$, we notice that
    \begin{align}
        &\sum_{i\in I_1'} d_\infty(D_1(i), D_2(f(i))) + \sum_{i\in I_1 \setminus I_1'} d_\infty(D_1(i), \Delta) + \sum_{i\in I_2 \setminus I_2'} d_\infty(D_2(i), \Delta) \\
        + &\sum_{i\in I_2'} d_\infty(D_2(i), D_3(g(i))) + \sum_{i\in I_2 \setminus I_2'} d_\infty(D_2(i), \Delta) + \sum_{i\in I_3 \setminus I_3'} d_\infty(D_3(i), \Delta) \\
        \geq &\sum_{i\in I_1'} d_\infty(D_1(i), D_3(g \circ f(i))) + \sum_{i\in I_1 \setminus I_1'} d_\infty(D_1(i), \Delta) + \sum_{i\in I_3 \setminus I_3'} d_\infty(D_3(i), \Delta)
    \end{align}
    as $ d_\infty(D_1(i), D_2(f(i))) + d_\infty(D_2(f(i)), D_2(g(f(i)))) \geq d_\infty(D_1(i), D_3(g \circ f(i))) $ using the triangle inequality of $ d_\infty $. Therefore, for all partial matchings $ f $ and $ g $ as described, we have $ \costp (f) + \costp (g) \geq \costp (g \circ f) $. Using the same reasoning, por $ p = \infty $ we also obtain $ \costi (f) + \costi (g) \geq \costi (g \circ f) $. Hence, we have verified that
    \begin{align}
        \wdp(D_1, D_2) + \wdp(D_2, D_3) \geq \wdp(D_1, D_3).
    \end{align}
\end{proof}

\section{The Hausdorff and Gromov-Hausdorff distances}

The Hausdorff distance is a way of measuring distances of different sets contained into a same metric space. This concept can be generalized defining a metric which allow us to measure distances between different metric spaces.

\begin{definition}[Hausdorff distance]
    Let $ (M, d) $ be a metric space, and let $ A \subseteq M $, $ B \subseteq M $ two compact subspaces of $ M $. Define the {\bf $r$-neighborhood} of a set $ S \subset M $ as
    $$
        U_r(S) \coloneq \left\{ x \in S \mid d(x, S) \leq r \right\}.
    $$
    The {\bf Hausdorff distance} can be defined as
    $$
        \dhf(A, B) \coloneq \inf \left\{ r > 0 \mid A \subset U_r(B) \text{ and } B \subset U_r(A) \right\}.
    $$
\end{definition}

\begin{definition}[Isometric metric spaces]
    Let $ (X, d_X), (Y, d_Y) $ be metric spaces. $ X $ and $ Y $ are said to be {\bf isometric} if there exists a bijective map $ f: X \to Y $ such that distances are preserved. That is, for all $ x_1, x_2 \in X $,
    $$
        d_X(x_1, x_2) = d_Y(f(x_1), f(x_2)).
    $$
\end{definition}

\begin{definition}[Gromov-Hausdorff distance] \label{def:dgh}
    Let $ (X, d_X), (Y, d_Y) $ be metric spaces. The {\bf Gromov-Hausdorff} distance is defined as
    \begin{equation}
        \dgh \coloneq \inf \left\{ r > 0 \mid \exists (Z, d_Z) \text{ metric space such that}, \exists X', Y' \subseteq Z, \dhf(X', Y') < r \right\},
    \end{equation}
    where $ X', Y' $ are isometric spaces to $ X $ and $ Y $ respectively.
\end{definition}

\begin{lemma}[Proposition 7.3.16, \cite{burago}] \label{lemma:gh-triangle}
    Gromov-Hausdorff distance satisfy the triangle inequality. That is, for any metric spaces $ X_1, X_2, X_3 $ it is verified that
    \begin{equation}
        \dgh(X_1, X_3) \leq \dgh(X_1, X_2) + \dgh(X_2, X_3).
    \end{equation}
\end{lemma}
\begin{proof}
    Let $ d_{12} $ be a metric over $ X_1 \cup X_2 $ and let $ d_{23} $ be a metric over $ X_2 \cup X_3 $. Over $ X_1 \cap X_3 $, define
    \begin{equation}
        d_{13} \coloneq \begin{cases}
        &d_{X_1}(x_1, x_3) \text{ if } x_1, x_3 \in X_1, \\
        &d_{X_2}(x_1, x_3) \text{ if } x_1, x_3 \in X_3, \\
        &\inf_{x_2 \in X_2} \{d_{12}(x_1, x_2) + d_{23}(x_2, x_3)\} \text{ if } x_1 \in X_1, x_3 \in X_3.
        \end{cases}
    \end{equation}
    For the first two cases we clearly have a metric. For the third one observe that taking $ x_1 \in X_1, x_3 \in X_3 $ and some $ x \in X_1 $ we have
    \begin{align}
        d_{13}(x_1, x) + d_{13}(x, x_3) &= d_{X_1}(x_1, x) + \inf_{x_2 \in X_2} \{d_{12}(x, x_2) + d_{23}(x_2, x_3)\} \\
        &= \inf_{x_2 \in X_2} \{d_{12}(x_1, x) + d_{12}(x, x_2) + d_{23}(x_2, x_3)\} \\
        &\geq \inf_{x_2 \in X_2} \{d_{12}(x_1, x_2) + d_{23}(x_2, x_3)\} \\
        &= d_{13}(x_1, x_3).
    \end{align}
    This implies, taking the corresponding metric $ d_{ij} $ where $ i, j = 1, 2, 3$, that 
    \begin{equation}
        \dhf(X_1, X_3) \leq \dhf(X_1, X_2) + \dhf(X_2, X_3),
    \end{equation}
    and, taking the infimum over the metrics $ d_{12} $ and $ d_{23} $ we have
    \begin{equation}
        \dgh(X_1, X_3) \leq \dgh(X_1, X_2) + \dgh(X_2, X_3).
    \end{equation}
\end{proof}

To check that Gromov-Hausdorff distance is actually a distance we first give a useful characterization in \ref{prop:gh-characterization}. It is expressed in terms of correspondance distortions.

\begin{definition}[Correspondance between sets]
    Given two sets $ X $ and $ Y $, a {\bf correspondance} between them is a set $ R \subseteq X \times Y $ verifying that for every $ x \in X $, there exists at least one $ y \in Y $ such that $(x, y) \in R $ and, for every $ y \in Y $, there exists an $ x \in X $ such that $(x, y) \in R $. 
\end{definition}

\begin{definition}[Distortion of a correspondance]
    Let $ (X, d_X), (Y, d_Y) $ be two metric spaces, and let $ R $ be a correspondance between them. The {\bf distortion} of $ R $ is defined as
    \begin{equation}
        \operatorname{dis}(R) \coloneq \sup \left\{|d_X(x, x') - d_Y(y, y')| \colon (x, y), (x', y') \in R\right\}.
    \end{equation}
\end{definition}

\begin{proposition}[Theorem 7.3.25, \cite{burago}] \label{prop:gh-characterization}
    Let $ (X, d_X), (Y, d_Y) $ be two metric spaces. The Gromov-Hausdorff distance between them can be characterized as
    \begin{equation}
        \dgh((X, d_X), (Y, d_Y)) = \frac{1}{2} \inf_{R} \operatorname{dis}(R).   
    \end{equation}
\end{proposition}
\begin{proof}
    Given $ r \geq \dgh(X, Y) $, for some metric space $(Z, d_Z)$, we can take $ X', Y' \subseteq Z $ such that $ X' $ and $ Y' $ are isometric embeddings of $ X $ and $ Y $ respectively and $ \dhf(X', Y') < r $ in $ Z $. Thus, we can see every element of $ X $ and $ Y $ as elements od $ Z $ trough some isometry. Therefore, we can define the correspondance
    \begin{equation}
        R \coloneq \{(x, y) \in X \times Y \colon d_Z(x, y) < r\}.
    \end{equation}
    The set $ R $ is actually a correspondance because the fact that $ \dhf(X', Y') < r $ implies that for every $ x \in X $ and every $ y \in Y $, $ d_Z(x, y) < r $, so every $ x $ and every $ y $ have some correspondance. Now, let $ (x, y), (x', y') \in R $. Using the triangle inequality of $ Z $ we have
    \begin{align}
        \operatorname{dis}(R) &\leq |d_X(x, x') - d_Y(y - y')| \\
        &= |d_Z(x, x') - d_Z(y - y')| \\
        &\leq |d_Z(x, y) + d_Z(y, x') - d_Z(y - y')| \\
        &\leq d_Z(x, y) + d_Z(x', y) + d_Z(y - y') \\
        &\leq d_Z(x, y) + d_Z(x', y') \leq 2r.
    \end{align}
    This shows
    \begin{equation}
        2 \dgh(X, Y) \geq \inf_{R} \operatorname{dis}(R).
    \end{equation}
    To see the reverse inequality, let $ R $ be any correspondance, and lets take $ \operatorname{dis}(R) = 2r $. Lets construct a metric space $ (Z, d_Z) $ formed by the disjoint union of spaces $ Z = X \cup Y $. For every $ z_1, z_2 \in Z $, we define $ d_Z $ as
    \begin{equation}
        d_Z(z_1, z_2) \coloneq \begin{cases}
            &d_X(z_1, z_2) \text{ if } z_1, z_2 \in X, \\
            &d_Y(z_1, z_2) \text{ if } z_1, z_2 \in Y, \\
            &\inf\{d_X(z_1, x') + r + d_Y(z_2, y') \colon (x', y') \in R\} \text{ if } z_1 \in X, z_2 \in Y.
        \end{cases}
    \end{equation}
    By definition, it is clear that $ d_Z $ respects isometrically both $ d_X $ and $ d_Y $. By the same reason $ d_Z(z_1, z_2) = d_Z(z_2, z_1) $  and $ d_Z(z_1, z_2) \geq 0 $ for every $ z_1, z_2 \in Z$, where $ d_Z(z_1, z_2) = 0 $ only if either $ z_1 = z_2 $ or $ r = 0$. To check the triangle inequality we take $ z_1, z_2, z_3 \in Z $. If either all three are elements of $ X $, or all three are elements of $ Y $, the inequality is verified as it is granted in $ X $ and $ Y $ with $ d_X $ and $ d_Y $ respectively. In case $ z_1, z_2 \in X $ and $ z_3 \in Y $ we can take some $ y \in Y $ such that $ (z_2, y) \in R $. Thus, we have
    \begin{align}
        d_Z(z_1, z_2) + d_Z(z_2, z_3) &\geq d_X(z_1, z_2) + d_X(z_2, z_2) + r + d_Y(z_3, y) \\
        &\geq d_X(z_1, z_2) + r + d_Y(z_3, y) \\
        &\geq d_Z(z_1, z_3).
    \end{align}
    Analogously, the argument follows for $ z_1 \in X $ and $ z_2, z_3 \in Y $. Thus, all is left to prove is to check $ \dhf(X, Y) < r $.

    ...
\end{proof}

\begin{definition}[Distortion of a map]
    Let $ (X, d_X), (Y, d_Y) $ be two metric spaces and let $ f \colon X \to Y $ an arbitrary map. The {\bf distortion} of $ f $ is defined as
    \begin{equation}
        \operatorname{dis}(f) \coloneq \sup_{x_1, x_2 \in X} |d_Y(d(x_1), f(x_2)) - d_X(x_1, x_2)|.
    \end{equation}
\end{definition}

\begin{definition}[$\e$-isometry]
    Let $ X $ and $ Y $ be two metric spaces and let $ \e > 0 $. A {\bf $\e$-isometry} between two metric spaces is a map $ f \colon X \to Y $ such that $ \operatorname{dis}(f) \leq \e $. The image $ f(X) $ is called an {\bf $\e$-net}.
\end{definition}

\begin{proposition}[Theorem 7.3.28.1, \cite{burago}] \label{prop:e-isometry}
    Let $ X $ and $ Y $ be two metric spaces and let $ \e > 0 $. If $ \dgh < \e $, them there exists a $ 2 \e -isometry $ from $ X $ to $ Y $.
\end{proposition}
\begin{proof}
    Let $ R $ be a correspondance between $ X $ and $ Y $. By \ref{prop:gh-characterization} it holds that $\operatorname{dis}(R) < 2 \e$. For every $ x \in X $ we choose some $ y \in Y $ such that $(x, y) \in R$ and define $ f(x) \coloneq y $. This defines a map $ f \colon X \to Y $. We then have
    \begin{equation}
        \operatorname{dis}(f) \leq \operatorname{dis}(R) < 2 \e.
    \end{equation}

    ...
\end{proof}

Up to this moment we have seen that Gromov-Hausdorff distance defines a pseudo-metric over the set of metric spaces. Note that if $ X $ and $ Y $ are isometric, directly of the definition ge get, $ \dgh(X, Y) = 0 $. To make Gromov-Hausdorff distance an actual metric we need to ask one more thing to our metric spaces. That is, to be compact. Denote
\begin{equation}
    \mathcal{X} \coloneq \{ (X, d_X) \colon (X, d_X) \text{ is a metric compact space} \}.
\end{equation}

\begin{theorem}[Theorem 7.3.30, \cite{burago}] \label{theorem:gh-distance}
    Gromov-Hausdorff distance is in fact a metric over the space of isometry clases of compact metric spaces.    
\end{theorem}
\begin{proof}
    We just seen that if $ X $ and $ Y $ are isometric, directly of the definition ge get, $ \dgh(X, Y) = 0 $. By definition, Gromov-Hausdorff distance is nonnegative and symmetric and, by Lemma \ref{lemma:gh-triangle}, it verifies the triangle inequality. It only remains to prove that given two metric spaces $ X, Y \in \mathcal{X} $, if $ \dgh(X, Y) = 0 $ then $ X $ and $ Y $ are isometric.  
    
    Let  $ X, Y \in \mathcal{X} $ such that $ \dgh(X, Y) = 0 $. By Proposition \ref{prop:e-isometry}, there exists a sequence of maps $ f_n \colon X \to Y $ such that $ \operatorname{dis}(f_n) \to 0 $. As $ X $ is compact, we can fix a countable dense set $ S \subset X $.
    
    ...
\end{proof}

In order to extend the scope of Gromov-Hausdorff distance we can endow our compact metric spaces with real-valued functions, which will still maintain good stability properties as we will see in Chapter \ref{chapter:gromov-hausdorf-stability}.

Denote the collection of such spaces as
\begin{align}
    \mathcal{X}_1 \coloneq \{ (X, d_x, f) \colon (X, d_x) \in \mathcal(X), f_X \colon X \to \R \text{ continuous}\}.
\end{align}

\begin{definition} \label{def:dgh1}
    Let $ X, Y \in \mathcal{X}_1 $. We extend the {\bf Gromov-Hausdorff distance over $ X_1 $} as
    \begin{equation}
        \dgh^1((X, d_X, f_X), (Y, d_Y, f_Y)) = \inf_{R} \max \left\{ \frac{1}{2} \operatorname{dis}(R), \|f_X - f_Y \|_{\ell^\infty} \right\}.   
    \end{equation}
\end{definition}

An analogous adaptation of Theorem \ref{theorem:gh-distance} and the previous results proofs that $ \dgh^1 $ defines a metric over the set of isomorphism clases od $ \mathcal{X}_1 $.